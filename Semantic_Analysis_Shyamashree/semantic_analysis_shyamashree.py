# -*- coding: utf-8 -*-
"""Semantic_Analysis_Shyamashree.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HxgUnM0DSwfUpDUV_Td9XlDnjavbYn_T
"""

import io
from google.colab import files

uploaded = files.upload()

import numpy as np
import pandas as pd

df = pd.read_csv("movies.csv")

df.head()

import spacy
import string
import re
from spacy.lang.en.stop_words import STOP_WORDS
spacy_nlp = spacy.load('en_core_web_sm')

# creating list of punctuations and stopwords
punctuations = string.punctuation
stop_words = spacy.lang.en.stop_words.STOP_WORDS

# function for data cleaning and processing

def text_cleaning(sentence):


  # eliminate digit
  sentence = re.sub(r'\+d','',sentence)
  # eliminate special characters
  sentence = re.sub(r'[^\w\s]','',sentence)
  #replace extra spaces with single space
  sentence = re.sub(' +',' ',sentence)
  #replace extra spaces with single space
  sentence = re.sub(' +',' ',sentence)
  # tokenize the text
  tokens = spacy_nlp(sentence)
  #lower, strip and lemmatize
  tokens = [word.lemma_.lower().strip() for word in tokens]
  #remove stopwords, and exclude words less than 2 characters
  tokens = [word for word in tokens if word not in stop_words and word not in punctuations and len(word) > 2]
  # return tokens
  return tokens

df['wiki_plot_tokenized'] = df['wiki_plot'].map(lambda x: text_cleaning(x))

df.head()

from gensim import corpora
import gensim

movie_plot = df['wiki_plot_tokenized']

# creating the term dictionary
dictionary = corpora.Dictionary(movie_plot)

# creating bag of words
corpus = [dictionary.doc2bow(desc) for desc in movie_plot]
word_frequencies = [[(dictionary[id], frequency) for id, frequency in line] for line in corpus[0:3]]

# finding the most inportant words
movie_tfidf_model = gensim.models.TfidfModel(corpus, id2word=dictionary)
movie_lsi_model = gensim.models.LsiModel(movie_tfidf_model[corpus], id2word=dictionary, num_topics=600)

# Serialize and Store the corpus locally for easy retrival whenver required.
gensim.corpora.MmCorpus.serialize('movie_tfidf_model_mm', movie_tfidf_model[corpus])
gensim.corpora.MmCorpus.serialize('movie_lsi_model_mm',movie_lsi_model[movie_tfidf_model[corpus]])

# Loading the indexed corpus
movie_tfidf_corpus = gensim.corpora.MmCorpus('movie_tfidf_model_mm')
movie_lsi_corpus = gensim.corpora.MmCorpus('movie_lsi_model_mm')

# Loading the MatrixSimilarity
from gensim.similarities import MatrixSimilarity
movie_index = MatrixSimilarity(movie_lsi_corpus, num_features = movie_lsi_corpus.num_terms)

from operator import itemgetter

def search_similar_movies(search_term):

    query_bow = dictionary.doc2bow(text_cleaning(search_term))
    query_tfidf = movie_tfidf_model[query_bow]
    query_lsi = movie_lsi_model[query_tfidf]

    movie_index.num_best = 6

    movies_list = movie_index[query_lsi]


    movies_list.sort(key=itemgetter(1), reverse=True)
    movie_names = []

    for j, movie in enumerate(movies_list):

        movie_names.append (
            {
                'Relevance': round((movie[1] * 100),2),
                'Movie Title': df['title'][movie[0]],
                'Movie Plot': df['wiki_plot'][movie[0]]
            }

        )
        if j == (movie_index.num_best-1):
            break

    return pd.DataFrame(movie_names, columns=['Relevance','Movie Title','Movie Plot'])

# search for movie tiles that are related to below search parameters
search_similar_movies('crime and drugs ')

search_similar_movies('Horror')



