# -*- coding: utf-8 -*-
"""NLP_Shyamashree.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HzD57c3wms5EziLEhz7-weNG4_BrdUMQ
"""

# Explore NLP and about NLTK Kit and spacy library. When you explore, prepare a google colab notebook. It contains your R&D with two examples.

'''
NLP:-
Natural language processing is a field of artificial intelligence and computer science that is concerned
with the interactions between computers and humans in natural language. The main goal of NLP is to develop
algorithms and models that enable computers to understand, interpret, generate and manipulate human languages.
Companies use it for several automated task, such as:
1. process,analyze and archive large documents.
2. analyze customer feedback or call centre recordings.
3. run chatbots for automated customer service.
4. answer who-what-when-where questions.
5. classify and extract text.

Sample of NLP preprocessing techniques:
Tokenization: Tokenization splits the raw text into a sequence of tokens(words). Tokenization is often the
first step of the NLP pipeline.

Bag-of-words models: Bag-of-words models treat documents as unordered collections of tokens or words.
Because they completely ignore word order. It will confuse a sentence such as "dog bites man" as "man bites dog".
Bag-of-words models are often used for efficiency reasons on large information retrieval tasks such as search engines.

Stop word removal: A "stop word" is a token that is ignored in later processing. They are typically short such as "a",
"an" and "the". Bag-of-words models and search engines often ignore stop words in order to reduce processing time and
storage within the database.

Stemming and lemmatization: Stemming is a process that stems or removes last few characters from a word, often leading to
incorrect meanings and spelling. Lemmatization considers the context and converts the word to its meaningful base form,
which is called Lemma. For example, stemming and lemmatizing the word "Caring" would return "Car" and "Care" respectively.

Part-of-speech tagging and syntactic parsing: Part-of-speech tagging is the process of labeling each word with its part of
speech. A Syntactic parser identifies how words combine to form phrases, clauses, and entire sentences.

'''

'''
NLTK Kit: Natural language processing is a field focuses on making human language usable by computer programs. NLTK, or Natural
Language Toolkit is a python package that we can use for NLP.
'''
! pip install nltk==3.5

import nltk
nltk.download('punkt')

# word tokenizing
from nltk.tokenize import sent_tokenize, word_tokenize
example_string = """
I am a student. She is a girl. He is a boy."""

sent_tokenize(example_string)

word_tokenize(example_string)

# Removing stop_words
nltk.download("stopwords")
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

text = "I am very to hear it. Can you tell me the reason?"

tokenize_text = word_tokenize(text)
print(tokenize_text)

stop_words = set(stopwords.words("english"))

filtered_text = [ word for word in tokenize_text if word.casefold() not in stop_words]
print(filtered_text)

# Stemming
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize

stemmer = PorterStemmer()

string_for_stemming = """
Words for stemming. It will stem few characters from a word."""

words = word_tokenize(string_for_stemming)
print(words)

stemmed_words = [stemmer.stem(word) for word in words]
print(stemmed_words)

# Tagging Parts of Speech
from nltk.tokenize import word_tokenize
text = """
Parts of speech tagging is the process of labeling each word with its part of
speech. """

tokenize_text = word_tokenize(text)
import nltk
nltk.download('averaged_perceptron_tagger')
nltk.pos_tag(tokenize_text)

'''
Spacy: spaCy is a free and open-source library with a lot of built-in capabilities.
Itâ€™s becoming increasingly popular for processing and analyzing data in the field of NLP.
'''
! pip install spacy

import spacy
nlp = spacy.load("en_core_web_sm")

introduction_doc = nlp("This  is about Natural Language Processing in spaCy." )

type(introduction_doc)

[token.text for token in introduction_doc]

for token in introduction_doc:
  print(token, token.idx)
print (token, token.idx)

# Visualization: Using displaCy
from spacy import displacy
displacy.serve(introduction_doc, style="dep")











